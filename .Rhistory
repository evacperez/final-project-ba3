## Using the `median` function, create a variable `numbers_median` that is the median of your vector `numbers`
numbers_median <- median(numbers)
## Create a vector `lower_numbers` that is the numbers 500:699
lower_numbers <- c(500:699)
## Create a vector `all_numbers` that combines your `lower_numbers` and `numbers` vectors
all_numbers <- c(lower_numbers, numbers)
### -------------------- Dates --------------------
## you may also consult 'lubridate' package for additional date-time functionality
## Use the `as.Date()` function to create a variable `today` that represents today's date
## You can pass in a character string of the day you wrote this, or you can get the current date
## Hint: check documentation for 'Sys.time' and 'Sys.Date'
today <- as.Date(Sys.Date())
## Create a variable `winter_break` that represents the first day of winter break (Dec 15, 2018).
## Make sure to use the `as.Date` function again
winter_break <- as.Date("2018/12/15")
## Create a variable `days_to_break` that is how many days until break (hint: subtract the dates!)
days_to_break <- difftime(winter_break, today)
## Define a function called `bday_intro` that takes in three arguments:
## a name, an age, and a character string for your next (upcoming) birthday.
## This method should return a character string of the format:
##  "Hello, my name is {name} and I'm {age} years old. In {N} days I'll be {new_age}"
## You should utilize your `make_introduction` function from Part 1, and compute {N} and {new_age} in your
## function
bday_intro <- function(name, age, bday) {
introduction <- make_introduction(name, age)
new_age <- age + 1
days_to_bday <- difftime(bday, Sys.time())
two_sentences <- str_c(introduction, " In ", days_to_bday, " days I'll be ", new_age)
return(two_sentences)
}
## Create a variable `my_bday_intro` using the `bday_intro` function, passing in `my_name`, `my_age`,
## and your upcoming birthday.
my_bday_intro <- bday_intro(my_name, my_age, "2018/10/31")
## Note: you may look up 'lubridate' package by Hadley Wickham for more convenient handling of dates
### -------------------- Challenge --------------------
## Write a function `RemoveDigits` that will remove all digits (i.e., 0 through 9) from all elements in a
## *vector of strings*.
# Digits to remove
digits <- c("0","1","2","3","4","5","6","7","8","9")
# Remove single digits from string
remove_digit <- function (string, digit) {
return(str_replace_all(string,digit,""))
}
# Iterates to remove all digits from string
remove_digits_function <- function(string) {
for (digit in digits) {
string = remove_digit(string, digit)
}
return(string)
}
# Iterates through strings
RemoveDigits <- function(string_vector) {
len <- length(string_vector)
vector <- ""
for (string in string_vector) {
vector = c(vector, lapply(string, remove_digits_function))
}
return (vector)
}
## Demonstrate that your approach is successful by passing a vector of courses to your function
## For example, RemoveDigits(c("INFO 201", "CSE 142", "mps-803c"))
# Voila!
RemoveDigits(c("INFO 201", "CSE 142", "mps-803c"))
## Write an if/else statement that checks to see if your vector has any digits. If it does have
## digits, print "Oh no!", if it does not then print "Yay!"
courses <- c("INFO", "CSE", "mps-c")
has_digits <- function(words) {
has_all_true <- grepl("[[:digit:]]", words)
if(!all(has_all_true)) {
return("Yay!")
} else {
return("Oh no!")
}
}
has_digits(courses)
install.packages("stringr")
has_digits(courses)
courses <- c("INFO5", "CSE", "mps-c")
has_digits(courses)
loud_intro <- toupper(my_intro)
## Create a new variable `quiet_intro`, which is your `my_intro` variable in all lower-case letters
quiet_intro <- tolower(my_intro)
loud_intro
quiet_intro
## Create a new variable `quiet_intro`, which is your `my_intro` variable in all lower-case letters
quiet_intro <- tolower(my_intro)
## Install and load the the `stringr` package, which has a variety of built in functions that make working
## with string variables easier.  You may read more about stringr in Wickham & Grolemund "R for Data Science"
## http://r4ds.had.co.nz/strings.html
install.packages("stringr")
install.packages("stringr")
source('~/Documents/a2-core-programming-jbongoco/assignment.R')
pwd
cd ~/Documents/a3-using-data-jbongoco
cd ~
first_names <- c("JoAnne", "Liezl", "Alexis", "Bianca", "Jacky", "Jin")
cd ~Documents/a3-using-data-jbongoco
first_names <- c("JoAnne", "Liezl", "Alexis", "Bianca", "Jacky", "Jin")
first_names <- c("JoAnne", "Liezl", "Alexis", "Bianca", "Jacky")
first_names
math_grades <- c(JoAnne=73, Liezl=88, Alexis=92, Bianca=78, Jacky=85)
math_grades
# Load libraries
library(ggplot2)
library(dplyr)
# Plots a graph and displays it to the user
# For demonstration purposes only, UNCOMMENT TO DISPLAY GRAPH
# Directory will be automatically set, no need to setwd() manually
# this.dir <- dirname(parent.frame(2)$ofile)
# setwd(this.dir)
# data <- read.csv("../data/UFOCoords.csv")
# print(make_chart_1(data))
# The actual function
# This function takes one parameter 'data', the data frame that contains
# the UFO data.
make_chart_1 <- function(data) {
ggplot(
data,
aes(
x = reorder(State,State, function(x)-length(x))
)
) +
geom_bar(aes(fill = (Country == "USA"))) +
scale_fill_manual(
values = c("red", "royalblue"),
name = "Country",
labels = c("Canada", "United States")
) +
scale_y_continuous(breaks=c(seq(0, 600, 50)), limits = c(0, 600)) +
xlab("States and Provinces") +
ylab("Occurence Frequency")
}
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
data <- read.csv("../data/UFOCoords.csv")
print(make_chart_1(data))
this.dir <- dirname(parent.frame(2)$ofile)
library("maps")
library("ggplot2")
install.packages("ggplot2")
install.packages("dplyr")
library(ggplot2)
library(dplyr)
install.packages("dplyr")
library(ggplot2)
library(dplyr)
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
data <- read.csv("../data/UFOCoords.csv")
print(make_chart_1(data))
usa_area <- map_data("state")
make_chart_2 <- function(dataset){
ggplot() +
geom_polygon(data = usa_area, aes(x = long, y = lat, group = group), fill = "Black", color = "white") +
geom_point(data = dataset, aes(x = lng, y = lat, fill = AM.PM, color = AM.PM), cex = .6) +
coord_map()
}
library("maps")
install.packages("maps")
library("maps")
library("ggplot2")
usa_area <- map_data("state")
make_chart_2 <- function(dataset){
ggplot() +
geom_polygon(data = usa_area, aes(x = long, y = lat, group = group), fill = "Black", color = "white") +
geom_point(data = dataset, aes(x = lng, y = lat, fill = AM.PM, color = AM.PM), cex = .6) +
coord_map()
}
View(usa_area)
map_data("state")
make_chart_2 <- function(dataset){
p <- ggplot() +
geom_polygon(data = usa_area, aes(x = long, y = lat, group = group), fill = "Black", color = "white") +
geom_point(data = dataset, aes(x = lng, y = lat, fill = AM.PM, color = AM.PM), cex = .6) +
coord_map()
}
map_data("state")
?map_data
dataset <- read.csv("../data/UFOCoords.csv")
parent.frame(2)$ofile
parent.frame(2)
parent.frame(2)$ofile
this.dir <- dirname(parent.frame(3)$ofile)
dirname(parent.frame(2)$ofile)
this.dir <- dirname(parent.frame(2)$ofile)
dirname(sys.frame(2)$ofile)
parent.frame
parent.frame(2)
colnames(parent.frame(2))
this.dir <- dirname(parent.frame(2)$ofile)
setwd(this.dir)
data <- read.csv("../data/UFOCoords.csv")
print(make_chart_1(data))
data <- read.csv("/Users/josellyanneongoco/Documents/a7-collaboration-ba3/data/UFOCoords.csv")
data <- read.csv("/Users/josellyanneongoco/Documents/a7-collaboration-ba3/data/UFOCoords.csv")
data$AM.PM
data$AM.PM == "AM"
summarise(am = nrows(data$AM.PM == "AM"))
summarise(am = nrow(data$AM.PM == "AM"))
AMvsPM <- data %>% group_by(AM.PM) summarise(number = n())
AMvsPM <- data %>% group_by(AM.PM) %>% summarise(number = n())
View(AMvsPM)
AMcount <- AMvsPM[1,1]
AMcount
AMvsPM[1,1]
View(AMcount)
AMcount <- AMvsPM[2,2]
AMcount
AMcount <- AMvsPM[2,2]
PMcount <- AMvsPM[2,3]
View(data)
shapes <- data %>% group_by(Shape) %>% summary (n = n())
shapes <- data %>% group_by(Shape) %>% summarize(n = n())
View(shapes)
shapes <- data %>% group_by(Shape) %>% tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
shapes
popular_shape <- shapes[0,0]
popular_shape <- shapes[1,1]
popular_shape
popular_shape <- shapes[2,1]
popular_shape
popular_shape <- shapes[1,2]
popular_shape
View(data)
popular_shape <- shapes[1,2]
popular_shape
popular_shape <- shapes[1,1]
popular_shape
data %>% filter(Country = "USA" & Shape = popular_shape)
data %>% filter(Country == "USA" & Shape == popular_shape)
data %>% filter(Country == "USA" && Shape == popular_shape)
data %>% filter_(Country == "USA" && Shape == popular_shape)
data %>% filter_("Country" == "USA" && "Shape" == popular_shape)
data %>% filter(Country == "USA")
typeof(popular_shape)
popular_shape
most_shape_state <- data %>% filter(Country == "USA" & Shape == "Light")
most_shape_state
most_shape_state <- data %>%
filter(Country == "USA" & Shape == "Light") %>%
group_by(State) %>%
tally(sort = T) %>%
ungroup() %>% arrange(desc(n))
most_shape_state
frequent_popular_shape_sights_usa <- function(data){
shapes <- data %>% group_by(Shape) %>% tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
popular_shape <- shapes[1,1]
most_shape_state_tally <- data %>%
filter(Country == "USA" & Shape == "Light") %>%
group_by(State) %>%
tally(sort = T) %>%
ungroup() %>% arrange(desc(n))
most_shape_state <- most_shape_state_tally[1,1]
return(most_shape_state)
}
popular_shape_count
shapes <- data %>% group_by(Shape) %>% tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
popular_shape_count <- shapes[1,2]
return(popular_shape_count)
popular_shape_count
most_shape_state
most_shape_state_tally[1,1]
most_shape_state_tally <- data %>%
filter(Country == "USA" & Shape == "Light") %>%
group_by(State) %>%
tally(sort = T) %>%
ungroup() %>% arrange(desc(n))
most_shape_state <- most_shape_state_tally[1,1]
most_shape_state
shapes <- data %>% group_by(Shape) %>% tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
shapes
View(shapes)
View(most_shape_state_tally)
typeof(popular_shape)
paste( unlist(popular_shape), collapse='')
popular_shape
paste( unlist(popular_shape), collapse='')
most_shape_state_tally <- data %>%
filter(Country == "USA" & Shape == "Light") %>%
group_by(State) %>%
tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
most_shape_state <- most_shape_state_tally[1,1]
most_shape_state
paste(unlist(most_shape_state), collapse='')
shapes <- data %>%
group_by(Shape) %>%
tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
popular_shape_count <- shapes[1,2]
paste(unlist(popular_shape_count), collapse='')
shapes <- data %>%
group_by(Shape) %>%
tally(sort = T) %>%
ungroup() %>%
arrange(desc(n))
popular_shape <- shapes[1,1]
paste(unlist(popular_shape), collapse='')
shiny::runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
library(dplyr)
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Downloads/meteor')
install.packages("plotly")
library(plotly)
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
?sep
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/a8-app-jbongoco/jbongoco-a8')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
# JCPenney Closing Northgate Mall Store in 2019 <br/>
[Northgate Mall](https://www.simon.com/mall/northgate-mall) is a shopping center in Seattle that is in the process of **redevelopment** into a mixed-used center, which means leaving some of its _old department stores_ to close next year. JCPenney is one of the three mall giants amongst Macy's and Nordstrom. Will JCPenney's absence strike at the heart of its beloved shoppers? _Only time will tell._ <br/> <br/>
![](imgs/JCPenney.jpg) <br/>
> A spokesman for the 116-year-old company said Monday the store will close sometime in 2019.
_Additional information regarding the redevelopment:_
- [JCPenny](https://www.jcpenney.com/) has been part of the [shopping center](https://www.seattletimes.com/seattle-news/from-the-archives-look-at-northgate-mall-in-the-1950s-when-it-was-an-outdoor-shopping-center/) since 1965 but chooses not to be part of the new redevelopment.
- The next door department stores, Macy's and Nordstrom, have not replied to any request on comments for [redevelopment plans](https://www.seattletimes.com/business/real-estate/northgate-mall-readies-huge-overhaul-with-office-and-housing-elements-as-north-seattle-neighborhood-transforms/).
- This new redevelopment entails an additional Link light rail stop adjacent to the mall as well as [plans](https://www.seattletimes.com/sports/hockey/nhl-seattle-announces-future-three-rink-180000-square-foot-practice-facility-at-northgate-mall/) for a $70 million, 180,000-square-foot practice facility to be built.
You can read the full article [here](https://www.seattletimes.com/business/jcpenney-closing-northgate-mall-store-in-2019/).
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/a8-app-jbongoco/jbongoco-a8')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
data <- NULL
runApp('Documents/final-project-ba3')
runApp('Documents/final-project-ba3')
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
data <- read.csv("./data/Expected years of schooling, female (years).csv", stringsAsFactors = FALSE)
data <- read.csv("./data/Expected years of schooling, male (years).csv", stringsAsFactors = FALSE)
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
df <- na.omit(data)
# Remove ranks (we don't want them)
df$HDI.Rank..2017.<- NULL
# Remove 'X' from column names for years
names(df)[2:29] <- substring(names(df)[2:29],2,5)
# Remove trailing and leading whitespace from Country names
df$Country <- trimws(df$Country)
# Filter for correct country
df <- df %>% filter(Country == femaleMaleCountry)
# Melt the years+values to Country
melt <- melt(df, id = "Country")
# Filter for correct year range
melt <- melt %>% filter(year >= yearStart & year <= yearEnd)
melt <- melt(df, id = "Country")
library(reshape)
library(reshape2)
melt <- melt(df, id = "Country")
# Filter for correct year range
melt <- melt %>% filter(year >= yearStart & year <= yearEnd)
yearStart <- 1990
yearEnd <- 2017
femaleMaleCountry <- "Canada"
df <- na.omit(data)
# Remove ranks (we don't want them)
df$HDI.Rank..2017.<- NULL
# Remove 'X' from column names for years
names(df)[2:29] <- substring(names(df)[2:29],2,5)
# Remove trailing and leading whitespace from Country names
df$Country <- trimws(df$Country)
# Filter for correct country
df <- df %>% filter(Country == femaleMaleCountry)
# Melt the years+values to Country
melt <- melt(df, id = "Country")
# Filter for correct year range
melt <- melt %>% filter(year >= yearStart & year <= yearEnd)
View(df)
View(data)
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
View(data)
data <- read.csv("./data/Expected years of schooling (years).csv", stringsAsFactors = FALSE)
df <- na.omit(data)
# Remove ranks (we don't want them)
df$HDI.Rank..2017.<- NULL
# Remove 'X' from column names for years
names(df)[2:29] <- substring(names(df)[2:29],2,5)
# Remove trailing and leading whitespace from Country names
df$Country <- trimws(df$Country)
# Filter for correct country
df <- df %>% filter(Country == femaleMaleCountry)
# Melt the years+values to Country
melt <- melt(df, id = "Country")
# Filter for correct year range
melt <- melt %>% filter(year >= yearStart & year <= yearEnd)
View(melt)
melt <- melt %>% filter(variable >= yearStart & variable <= yearEnd)
melt <- melt(df, id = "Country")
melt <- melt %>% filter(variable >= 2000 & variable <= 2010)
melt <- melt(df, id = "Country")
# Filter for correct year range
melt <- melt %>% filter(variable > 2000 & variable < 2010)
melt <- melt(df, id = "Country")
typeof(melt$variable)
melt <- melt %>% filter(variable > 2000)
melt <- melt(df, id = "Country")
melt$variable <- as.numeric(melt$variable)
melt <- melt %>% filter(variable > 2000)
melt <- melt(df, id = "Country")
melt$variable <- as.numeric(melt$variable)
View(melt)
melt <- melt(df, id = "Country")
melt$variable <-as.numeric(as.character(melt$variable))
melt <- melt(df, id = "Country")
View(melt)
melt$variable <-as.numeric(as.character(melt$variable))
melt <- melt %>% filter(variable >= 2000)
runApp()
data <- read.csv("./data/Expected years of schooling, female (years).csv", stringsAsFactors = FALSE)
View(data)
df <- na.omit(data)
df$HDI.Rank..2017.<- NULL
names(df)[2:29] <- substring(names(df)[2:29],2,5)
runApp()
runApp()
